{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path = 'Semantic-Aware-Attention-Based-Deep-Object-Co-segmentation/'\n",
    "label_save_path = 'Data_ham/HAM_LABELS.npy'\n",
    "\n",
    "img_path1 = 'Data_ham/HAM10000_images_part_1/'\n",
    "img_path2 = 'Data_ham/HAM10000_images_part_2/'\n",
    "gt_path = 'Data_ham/HAM_MASK/'\n",
    "el_path = 'Data_ham/HAM_ellipse/'\n",
    "grab_path = 'Data_ham/HAM_grabcut/'\n",
    "\n",
    "acoseg_path = 'checkpoints/ham_coseg.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "%matplotlib inline\n",
    "import skimage, skimage.transform\n",
    "import sys\n",
    "sys.path.append(code_path)\n",
    "from torchvision import models\n",
    "import scipy.io as sio\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "# CUDA flag. Speed-up due to CUDA is mostly noticable for large batches.\n",
    "cuda = True\n",
    "from PIL import Image,ImageDraw\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, Scale, ToPILImage\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "#from skimage import io, transform\n",
    "from model import *\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as albu\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torchvision.models.resnet as resnet_util\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from random import shuffle\n",
    "from util import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dir = np.array(sorted(os.listdir(img_path1)+os.listdir(img_path2)))\n",
    "gt_dir = np.array(sorted(os.listdir(gt_path)))\n",
    "el_dir = np.array(sorted(os.listdir(el_path)))\n",
    "grab_dir = np.array(sorted(os.listdir(grab_path)))\n",
    "\n",
    "wrong=[1736, 3925, 5513]\n",
    "im_dir=np.delete(im_dir,wrong)\n",
    "gt_dir=np.delete(gt_dir,wrong)\n",
    "\n",
    "labels=np.load(label_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "ind = np.arange(len(im_dir))\n",
    "shuffle(ind)\n",
    "tr = int(len(ind)*0.7)\n",
    "val = tr+int(len(ind)*0.2)\n",
    "im_train_dir = im_dir[ind[:tr]]\n",
    "gt_train_dir = gt_dir[ind[:tr]]\n",
    "el_train_dir = el_dir[ind[:tr]]\n",
    "grab_train_dir = grab_dir[ind[:tr]]\n",
    "train_label=labels[ind[:tr]]\n",
    "\n",
    "im_val_dir = im_dir[ind[tr:val]]\n",
    "gt_val_dir = gt_dir[ind[tr:val]]\n",
    "el_val_dir = el_dir[ind[tr:val]]\n",
    "grab_val_dir = grab_dir[ind[tr:val]]\n",
    "val_label=labels[ind[tr:val]]\n",
    "\n",
    "im_test_dir = im_dir[ind[val:]]\n",
    "gt_test_dir = gt_dir[ind[val:]]\n",
    "el_test_dir = el_dir[ind[val:]]\n",
    "grab_test_dir = grab_dir[ind[val:]]\n",
    "test_label=labels[ind[val:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [albu.Resize(120,120),albu.PadIfNeeded(384, 480)]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return torch.from_numpy(x.transpose(2, 0, 1).astype('float32'))\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir,\n",
    "            masks_path,\n",
    "            label,\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.images_fps = []\n",
    "        for image_id in images_dir:\n",
    "            if os.path.isfile(os.path.join(img_path1, image_id)):\n",
    "                self.images_fps.append(os.path.join(img_path1, image_id))\n",
    "            else:\n",
    "                self.images_fps.append(os.path.join(img_path2, image_id))\n",
    "        self.masks_fps = [os.path.join(masks_path, mask_id) for mask_id in masks_dir]\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.label=label\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        mask = cv2.imread(self.masks_fps[i],0)/255.0\n",
    "        mask = np.expand_dims(mask,2)\n",
    "        sample = self.augmentation(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask'].reshape(384,480,1)\n",
    "        sample = self.preprocessing(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        return image, mask, torch.tensor(self.label[i])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.masks_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "propro=get_preprocessing(get_preprocessing_fn('resnet101', pretrained='imagenet'))\n",
    "train_dataset = Dataset(\n",
    "    im_train_dir, \n",
    "    el_train_dir, #grab_train_dir, \n",
    "    el_path, #grab_path,\n",
    "    train_label,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=propro\n",
    ")\n",
    "val_dataset = Dataset(\n",
    "    im_val_dir, \n",
    "    el_val_dir, #grab_val_dir, \n",
    "    el_path, #grab_path,\n",
    "    val_label,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=propro\n",
    ")\n",
    "test_dataset = Dataset(\n",
    "    im_test_dir, \n",
    "    gt_test_dir, \n",
    "    gt_path,\n",
    "    test_label,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=propro\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=bs, shuffle=True,\n",
    "                                             num_workers=1,drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=bs, shuffle=True,\n",
    "                                             num_workers=1,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=bs, shuffle=False,\n",
    "                                             num_workers=1,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACoseg Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dice_iou(pred, mas, iou, dice):\n",
    "    # iou, dice = [], []\n",
    "    for i in range(len(pred)):\n",
    "        pre = pred[i,132:252,180:300].numpy()\n",
    "        gt = mas[i,132:252,180:300].numpy()\n",
    "        inter = np.logical_and(pre==1,gt==1)\n",
    "        union  = np.logical_or(pre==1,gt==1)\n",
    "        iou.append(np.sum(inter)/np.sum(union))\n",
    "        dice.append(np.sum(pre[gt==1])*2.0 / (np.sum(pre) + np.sum(gt)))\n",
    "    # return iou, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, sam in enumerate(train_loader):\n",
    "        data = sam[0].float().cuda()\n",
    "        target = sam[1].float().cuda()\n",
    "\n",
    "        data_upsampled = data\n",
    "        target_upsampled = target\n",
    "\n",
    "        # go pair by pair\n",
    "        sub_batch_id = 0\n",
    "        for ind in [i*COSEG_BATCH_SIZE for i in range(int(data_upsampled.shape[0]/COSEG_BATCH_SIZE))]:\n",
    "            b1 = (ind,ind+COSEG_BATCH_SIZE)\n",
    "            if ind+COSEG_BATCH_SIZE >= data_upsampled.shape[0]:\n",
    "                b2 = (0,COSEG_BATCH_SIZE)\n",
    "            else:\n",
    "                b2 = (ind+COSEG_BATCH_SIZE,ind+COSEG_BATCH_SIZE*2)\n",
    "\n",
    "            output1, output2 = new_coseg_model(data_upsampled[b1[0]:b1[1]], data_upsampled[b2[0]:b2[1]])\n",
    "\n",
    "            loss1 = criterion(sig(output1.squeeze()), target_upsampled[b1[0]:b1[1]].squeeze(1))\n",
    "            loss2 = criterion(sig(output2.squeeze()), target_upsampled[b2[0]:b2[1]].squeeze(1))\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(new_coseg_model.parameters(), 0.05)\n",
    "            optimizer.step()\n",
    "            \n",
    "            out1 = sig(output1.squeeze()).cpu()\n",
    "            out2 = sig(output2.squeeze()).cpu()\n",
    "\n",
    "            pred1 = torch.empty(out1.shape)\n",
    "            pred2 = torch.empty(out2.shape)\n",
    "            pred1[out1>=0.5]=1\n",
    "            pred1[out1<0.5]=0\n",
    "            pred2[out2>=0.5]=1\n",
    "            pred2[out2<0.5]=0\n",
    "            \n",
    "            iou, dice = [], []    \n",
    "            get_dice_iou(pred1, target_upsampled[b1[0]:b1[1]].squeeze(1).long().cpu(), iou, dice)\n",
    "            get_dice_iou(pred2, target_upsampled[b2[0]:b2[1]].squeeze(1).long().cpu(), iou, dice)\n",
    "\n",
    "            avg_iou=np.mean(iou)\n",
    "            avg_dice=np.mean(dice)\n",
    "    \n",
    "            BATCH_ID = batch_idx * N_SUB_BATCHES + sub_batch_id\n",
    "            print(\"Epoch %s: Batch %i/%i Loss %.2f iou %.2f dice %.2f\" % (epoch, BATCH_ID,N_BATCHES, loss.item(), avg_iou, avg_dice))\n",
    "\n",
    "            logs['loss'] = loss.item()\n",
    "            logs['iou'] = avg_iou \n",
    "            logs['dice'] = avg_dice\n",
    "\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()\n",
    "            \n",
    "def valid(epoch):\n",
    "    val_loss=[]\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sam in enumerate(val_loader):\n",
    "            data = sam[0].float().cuda()\n",
    "            target = sam[1].float().cuda()\n",
    "\n",
    "            data_upsampled = data\n",
    "            target_upsampled = target\n",
    "\n",
    "            # go pair by pair\n",
    "            sub_batch_id = 0\n",
    "            for ind in [i*COSEG_BATCH_SIZE for i in range(int(data_upsampled.shape[0]/COSEG_BATCH_SIZE))]:\n",
    "                b1 = (ind,ind+COSEG_BATCH_SIZE)\n",
    "                if ind+COSEG_BATCH_SIZE >= data_upsampled.shape[0]:\n",
    "                    b2 = (0,COSEG_BATCH_SIZE)\n",
    "                else:\n",
    "                    b2 = (ind+COSEG_BATCH_SIZE,ind+COSEG_BATCH_SIZE*2)\n",
    "\n",
    "                output1, output2 = new_coseg_model(data_upsampled[b1[0]:b1[1]], data_upsampled[b2[0]:b2[1]])\n",
    "\n",
    "                loss1 = criterion(sig(output1.squeeze()), target_upsampled[b1[0]:b1[1]].squeeze(1))\n",
    "                loss2 = criterion(sig(output2.squeeze()), target_upsampled[b2[0]:b2[1]].squeeze(1))\n",
    "\n",
    "                loss = loss1 + loss2\n",
    "                val_loss.append(loss.item())\n",
    "                \n",
    "    avg_loss=np.mean(val_loss)\n",
    "    print('Val loss for epoch %s is %.f'%(epoch,avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cosegmentation network\n",
    "criterion = nn.BCELoss()\n",
    "criterion.cuda()\n",
    "\n",
    "new_coseg_model = model1().cuda()\n",
    "#checkpoint = torch.load('/misc/vlgscratch4/LakeGroup/shared_data/deep_lesion/checkpoints/res_coseg.pt')\n",
    "#model.load_state_dict(checkpoint['model_dict'])\n",
    "#new_coseg_model = new_coseg_model.train()\n",
    "\n",
    "sig = nn.Sigmoid()\n",
    "optimizer = Adam(new_coseg_model.parameters(), lr=1e-5)\n",
    "\n",
    "COSEG_BATCH_SIZE = 4\n",
    "N_LARGE_BATCHES = len(train_loader)\n",
    "BATCH_SIZE = 8\n",
    "N_SUB_BATCHES = BATCH_SIZE // COSEG_BATCH_SIZE\n",
    "\n",
    "N_BATCHES = N_SUB_BATCHES * N_LARGE_BATCHES\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "logs = {}\n",
    "\n",
    "min_val_loss=float('inf')\n",
    "max_patient=5\n",
    "m=0\n",
    "for epoch in range(50):\n",
    "    print('Epoch ', epoch)\n",
    "    new_coseg_model.train()\n",
    "    train(epoch)\n",
    "    \n",
    "    new_coseg_model.eval()\n",
    "    new_val_loss = valid(epoch)\n",
    "    if new_val_loss < min_val_loss:\n",
    "        min_val_loss = new_val_loss\n",
    "        torch.save({'model_dict': new_coseg_model.state_dict()},acoseg_path)\n",
    "        print('model saved')\n",
    "        m=0\n",
    "    else:\n",
    "        m+=1\n",
    "        if m >= max_patient:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACoseg Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pre-trained model\n"
     ]
    }
   ],
   "source": [
    "new_coseg_model = model1().cuda()\n",
    "\n",
    "checkpoint = torch.load(acoseg_path)\n",
    "new_coseg_model.load_state_dict(checkpoint['model_dict'])\n",
    "print('loaded pre-trained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 0.2855\n"
     ]
    }
   ],
   "source": [
    "sig = nn.Sigmoid()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "test_loss=[]\n",
    "mas = torch.tensor([])\n",
    "res = torch.tensor([])\n",
    "\n",
    "new_coseg_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, sam in enumerate(test_loader):\n",
    "        #print(batch_idx)\n",
    "        data = sam[0].float().cuda()\n",
    "        target = sam[1].float().cuda()\n",
    "        \n",
    "        output1, output2 = new_coseg_model(data[0:4], data[4:8])\n",
    "        output1 = sig(output1)\n",
    "        output2 = sig(output2)\n",
    "               \n",
    "        loss = criterion(output1.squeeze(), target[0:4].squeeze(1)) + criterion(output2.squeeze(), target[4:8].squeeze(1))\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        mas = torch.cat((mas,target.cpu()),dim=0)\n",
    "        res = torch.cat((res,output1.cpu()),dim=0)\n",
    "        res = torch.cat((res,output2.cpu()),dim=0)\n",
    "    avg_test_loss = np.mean(test_loss)\n",
    "    print('Test loss = {:.{prec}f}'.format(avg_test_loss, prec=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test IOU is 0.8389593173963711\n",
      "Best test DICE is 0.9083988489902314\n",
      "Test center error is 2.405524349475326\n",
      "Test circumstance error is 10.2995\n",
      "Test AVD is  9.520744619325107\n",
      "Test VS is  0.9272665906244174\n"
     ]
    }
   ],
   "source": [
    "mas = np.array(mas)\n",
    "res = np.array(res)\n",
    "\n",
    "from copy import deepcopy\n",
    "pred = deepcopy(res) \n",
    "pred[res>=0.5]=1\n",
    "pred[res<0.5]=0\n",
    "\n",
    "cut_pred = []\n",
    "cut_mas = []\n",
    "for i in range(pred.shape[0]):\n",
    "    cut_pred.append(np.uint8(pred[i,0,132:252,180:300]))\n",
    "    cut_mas.append(np.uint8(mas[i,0,132:252,180:300]))\n",
    "metrics(cut_mas,cut_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
